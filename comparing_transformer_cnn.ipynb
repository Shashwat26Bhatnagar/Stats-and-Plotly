{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn6FU23AlJRwS4NA83zzub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashwat26Bhatnagar/Stats-and-Plotly/blob/main/comparing_transformer_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "api = KaggleApi()\n",
        "# we write to the current directory with './'\n",
        "api.dataset_download_files('karakaggle/kaggle-cat-vs-dog-dataset')"
      ],
      "metadata": {
        "id": "j0Uu9ostpfIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNSIZbcWpRGz"
      },
      "outputs": [],
      "source": [
        "!unzip -qq kaggle-cat-vs-dog-dataset.zip\n",
        "!rm -r kaggle-cat-vs-dog-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RustamyF/vision-transformer.git\n",
        "!mv vision-transformer/vision_tr ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZtxWti-vRav",
        "outputId": "549c0d17-0e27-48bb-db49-ebaeca0bdc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision-transformer'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 43 (delta 14), reused 37 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (43/43), 128.71 KiB | 9.90 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "    def __init__(self):\n",
        "        self.cat_path = 'kagglecatsanddogs_3367a/PetImages/Cat'\n",
        "        self.dog_path = 'kagglecatsanddogs_3367a/PetImages/Dog'\n",
        "\n",
        "    def delete_non_jpeg_files(self, directory):\n",
        "        for filename in os.listdir(directory):\n",
        "            if not filename.endswith('.jpg') and not filename.endswith('.jpeg'):\n",
        "                file_path = os.path.join(directory, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                        os.unlink(file_path)\n",
        "                    elif os.path.isdir(file_path):\n",
        "                        shutil.rmtree(file_path)\n",
        "                    print('deleted', file_path)\n",
        "                except Exception as e:\n",
        "                    print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "    def data(self):\n",
        "        self.delete_non_jpeg_files(self.dog_path)\n",
        "        self.delete_non_jpeg_files(self.cat_path)\n",
        "\n",
        "        dog_list = os.listdir(self.dog_path)\n",
        "        dog_list = [(os.path.join(self.dog_path, i), 1) for i in dog_list]\n",
        "\n",
        "        cat_list = os.listdir(self.cat_path)\n",
        "        cat_list = [(os.path.join(self.cat_path, i), 0) for i in cat_list]\n",
        "\n",
        "        total_list = cat_list + dog_list\n",
        "\n",
        "        train_list, test_list = train_test_split(total_list, test_size=0.2)\n",
        "        train_list, val_list = train_test_split(train_list, test_size=0.2)\n",
        "        print('train list', len(train_list))\n",
        "        print('test list', len(test_list))\n",
        "        print('val list', len(val_list))\n",
        "        return train_list, test_list, val_list\n",
        "\n",
        "\n",
        "# data Augumentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "class dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    # dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    # load an one of images\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed, label"
      ],
      "metadata": {
        "id": "66GOBfDHvYsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cnn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Cnn, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(3 * 3 * 64, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(10, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "IhbUkKLgvgib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    lr = 0.001  # learning_rate\n",
        "    batch_size = 800  # we will use mini-batch method\n",
        "    epochs = 10  # How much to train a model\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    torch.manual_seed(1234)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    load_data = LoadData()\n",
        "\n",
        "    train_list, test_list, val_list = load_data.data()\n",
        "\n",
        "    train_data = dataset(train_list, transform=transform)\n",
        "    test_data = dataset(test_list, transform=transform)\n",
        "    val_data = dataset(val_list, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Cnn().to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for data, label in train_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = ((output.argmax(dim=1) == label).float().mean())\n",
        "            epoch_accuracy += acc / len(train_loader)\n",
        "            epoch_loss += loss / len(train_loader)\n",
        "\n",
        "        print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch + 1, epoch_accuracy, epoch_loss))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_val_accuracy = 0\n",
        "            epoch_val_loss = 0\n",
        "            for data, label in val_loader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                val_output = model(data)\n",
        "                val_loss = criterion(val_output, label)\n",
        "\n",
        "                acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
        "                epoch_val_accuracy += acc / len(val_loader)\n",
        "                epoch_val_loss += val_loss / len(val_loader)\n",
        "\n",
        "            print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch + 1, epoch_val_accuracy, epoch_val_loss))"
      ],
      "metadata": {
        "id": "iIBMBc8Vvi9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e9b4c8-a047-41ad-aabc-1c31a71a1efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "deleted kagglecatsanddogs_3367a/PetImages/Dog/Thumbs.db\n",
            "deleted kagglecatsanddogs_3367a/PetImages/Cat/Thumbs.db\n",
            "train list 15973\n",
            "test list 4992\n",
            "val list 3994\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1, train accuracy : 0.5952703356742859, train loss : 0.6677873730659485\n",
            "Epoch : 1, val_accuracy : 0.6201624870300293, val_loss : 0.648315966129303\n",
            "Epoch : 2, train accuracy : 0.6494419574737549, train loss : 0.6236229538917542\n",
            "Epoch : 2, val_accuracy : 0.6617305278778076, val_loss : 0.6134589910507202\n",
            "Epoch : 3, train accuracy : 0.6799637675285339, train loss : 0.5966665148735046\n",
            "Epoch : 3, val_accuracy : 0.6917853355407715, val_loss : 0.5852375030517578\n",
            "Epoch : 4, train accuracy : 0.7065808176994324, train loss : 0.5656211972236633\n",
            "Epoch : 4, val_accuracy : 0.7040126323699951, val_loss : 0.563623309135437\n",
            "Epoch : 5, train accuracy : 0.7173627614974976, train loss : 0.5466001033782959\n",
            "Epoch : 5, val_accuracy : 0.7153400182723999, val_loss : 0.5492264628410339\n",
            "Epoch : 6, train accuracy : 0.7237837314605713, train loss : 0.5359316468238831\n",
            "Epoch : 6, val_accuracy : 0.7210862636566162, val_loss : 0.5410972833633423\n",
            "Epoch : 7, train accuracy : 0.7359951734542847, train loss : 0.5190773010253906\n",
            "Epoch : 7, val_accuracy : 0.7363306283950806, val_loss : 0.521019458770752\n",
            "Epoch : 8, train accuracy : 0.7470401525497437, train loss : 0.5076072812080383\n",
            "Epoch : 8, val_accuracy : 0.7405824661254883, val_loss : 0.5198209285736084\n",
            "Epoch : 9, train accuracy : 0.7553890347480774, train loss : 0.4992372393608093\n",
            "Epoch : 9, val_accuracy : 0.7516429424285889, val_loss : 0.5064876675605774\n",
            "Epoch : 10, train accuracy : 0.7566637992858887, train loss : 0.49184533953666687\n",
            "Epoch : 10, val_accuracy : 0.7516297101974487, val_loss : 0.5003613233566284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch\n",
        "!pip install -U torchvision\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rcopUk7p6CK",
        "outputId": "7b1d72a8-3c21-42c6-8e0a-933e90952993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->torchvision) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from vision_tr.simple_vit import ViT\n",
        "# from vit_pytorch.efficient import ViT\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "# from linformer import Linformer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "# from vit_pytorch.efficient import ViT"
      ],
      "metadata": {
        "id": "yekRrIjFvHnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cat_path = 'kagglecatsanddogs_3367a/PetImages/Cat'\n",
        "        self.dog_path = 'kagglecatsanddogs_3367a/PetImages/Dog'\n",
        "\n",
        "    def delete_non_jpeg_files(self, directory):\n",
        "        for filename in os.listdir(directory):\n",
        "            if not filename.endswith('.jpg') and not filename.endswith('.jpeg'):\n",
        "                file_path = os.path.join(directory, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                        os.unlink(file_path)\n",
        "                    elif os.path.isdir(file_path):\n",
        "                        shutil.rmtree(file_path)\n",
        "                    print('deleted', file_path)\n",
        "                except Exception as e:\n",
        "                    print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "    def data(self):\n",
        "        self.delete_non_jpeg_files(self.dog_path)\n",
        "        self.delete_non_jpeg_files(self.cat_path)\n",
        "\n",
        "        dog_list = os.listdir(self.dog_path)\n",
        "        dog_list = [(os.path.join(self.dog_path, i), 1) for i in dog_list]\n",
        "\n",
        "        cat_list = os.listdir(self.cat_path)\n",
        "        cat_list = [(os.path.join(self.cat_path, i), 0) for i in cat_list]\n",
        "\n",
        "        total_list = cat_list + dog_list\n",
        "\n",
        "        train_list, test_list = train_test_split(total_list, test_size=0.2)\n",
        "        train_list, val_list = train_test_split(train_list, test_size=0.2)\n",
        "        print('train list', len(train_list))\n",
        "        print('test list', len(test_list))\n",
        "        print('val list', len(val_list))\n",
        "        return train_list, test_list, val_list\n",
        "\n",
        "\n",
        "# data Augumentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "Y5nkSJVxMlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    # dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    # load an one of images\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed, label\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Training settings\n",
        "    batch_size = 64\n",
        "    epochs = 20\n",
        "    lr = 3e-5\n",
        "    gamma = 0.7\n",
        "    seed = 42\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    torch.manual_seed(1234)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    load_data = LoadData()\n",
        "\n",
        "    train_list, test_list, val_list = load_data.data()\n",
        "\n",
        "    train_data = dataset(train_list, transform=transform)\n",
        "    test_data = dataset(test_list, transform=transform)\n",
        "    val_data = dataset(val_list, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
        "    model = ViT(\n",
        "        image_size=224,\n",
        "        patch_size=32,\n",
        "        num_classes=2,\n",
        "        dim=128,\n",
        "        depth=12,\n",
        "        heads=8,\n",
        "        mlp_dim=1024,\n",
        "        dropout=0.1,\n",
        "        emb_dropout=0.1,\n",
        "    ).to(device)\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "    epochs = 20\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for data, label in train_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = ((output.argmax(dim=1) == label).float().mean())\n",
        "            epoch_accuracy += acc / len(train_loader)\n",
        "            epoch_loss += loss / len(train_loader)\n",
        "\n",
        "        print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch + 1, epoch_accuracy, epoch_loss))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_val_accuracy = 0\n",
        "            epoch_val_loss = 0\n",
        "            for data, label in val_loader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                val_output = model(data)\n",
        "                val_loss = criterion(val_output, label)\n",
        "\n",
        "                acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
        "                epoch_val_accuracy += acc / len(val_loader)\n",
        "                epoch_val_loss += val_loss / len(val_loader)\n",
        "\n",
        "            print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch + 1, epoch_val_accuracy, epoch_val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBKayyLzMnCW",
        "outputId": "f4155359-d03e-47f9-ae5d-79827d133767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "train list 15973\n",
            "test list 4992\n",
            "val list 3994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default weights\n",
        "pretrained_weights = torchvision.models.ViT_b_16_Weights.DEFAULT\n",
        "\n",
        "# Model\n",
        "vit = vit_b_16(weights=pretrained_weights).to(device)\n",
        "\n",
        "for parameter in vit.parameters():\n",
        "  parameter.requires_grad=False\n",
        "\n",
        "# Change last layer\n",
        "vit.heads = nn.Linear(in_features=768, out_features=10)\n",
        "\n",
        "# Auto Transforms\n",
        "vit_transforms = pretrained_weights.transforms()"
      ],
      "metadata": {
        "id": "tQm_nTd8pi79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}